# Generative Adversarial Networks 

[Paper] Generative Adversarial Nets : https://arxiv.org/pdf/1406.2661.pdf  
[Tutotial] GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow : https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f

Examples of GANs :
![Alt Text](https://cdn-images-1.medium.com/max/1000/1*nKe_kwZoefrELGHh06sbuw.jpeg)

Some of the most relevant GAN pros and cons for the are:  
- They currently generate the sharpest images  
- They are easy to train (since no statistical inference is required), and only back-propogation is needed to obtain gradients  
- GANs are difficult to optimize due to unstable training dynamics.  
- No statistical inference can be done with them (except here): GANs belong to the class of direct implicit density models; they model p(x) without explicitly defining the p.d.f.  

## Understanding a GAN
![Alt Text](https://cdn-images-1.medium.com/max/1000/1*5rMmuXmAquGTT-odw-bOpw.jpeg)
Generative Adversarial Networks are composed of two models:  
- The first model is called a Generator and it aims to generate new data similar to the expected one. The Generator could be asimilated to a human art forger, which creates fake works of art.  
- The second model is named the Discriminator. This model’s goal is to recognize if an input data is ‘real’ — belongs to the original dataset — or if it is ‘fake’ — generated by a forger. In this scenario, a Discriminator is analogous to the police (or an art expert), which tries to detect artworks as truthful or fraud.  
   
The Generator (forger) needs to learn how to create data in such a way that the Discriminator isn’t able to distinguish it as fake anymore. The competition between these two teams is what improves their knowledge, until the Generator succeeds in creating realistic data.

A neural network G(z, θ₁) is used to model the Generator mentioned above. It’s role is mapping input noise variables z to the desired data space x (say images). Conversely, a second neural network D(x, θ₂) models the discriminator and outputs the probability that the data came from the real dataset, in the range (0,1). In both cases, θᵢ represents the weights or parameters that define each neural network. 

As a result, the Discriminator is trained to correctly classify the input data as either real or fake. This means it’s weights are updated as to maximize the probability that any real data input x is classified as belonging to the real dataset, while minimizing the probability that any fake image is classified as belonging to the real dataset. In more technical terms, the loss/error function used maximizes the function D(x), and it also minimizes D(G(z)).

Furthermore, the Generator is trained to fool the Discriminator by generating data as realistic as possible, which means that the Generator’s weight’s are optimized to maximize the probability that any fake image is classified as belonging to the real datase. Formally this means that the loss/error function used for this network maximizes D(G(z)).

In practice, the logarithm of the probability (e.g. log D(…)) is used in the loss functions instead of the raw probabilies, since using a log loss heavily penalises classifiers that are confident about an incorrect classification.

After several steps of training, if the Generator and Discriminator have enough capacity (if the networks can approximate the objective functions), they will reach a point at which both cannot improve anymore. At this point, the generator generates realistic synthetic data, and the discriminator is unable to differentiate between the two types of input.

## Training a GAN
Since both the generator and discriminator are being modeled with neural, networks, agradient-based optimization algorithm can be used to train the GAN. In our coding example we’ll be using stochastic gradient descent, as it has proven to be succesfull in multiple fields.

![Alt Text](https://cdn-images-1.medium.com/max/1250/1*OZ62_qvC6GAIH7CdajSRqg.png)

The fundamental steps to train a GAN can be described as following:
- Sample a noise set and a real-data set, each with size m.
- Train the Discriminator on this data.
- Sample a different noise subset with size m.
- Train the Generator on this data.
- Repeat from Step 1.
